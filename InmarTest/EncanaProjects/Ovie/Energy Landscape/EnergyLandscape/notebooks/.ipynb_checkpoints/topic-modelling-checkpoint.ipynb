{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELM Proof of Concept #2 : Topic Modelling\n",
    "\n",
    "* October 5th, 2018\n",
    "* Ryan Kazmerik, Strategic EIM\n",
    "\n",
    "## Hypothesis\n",
    "Topic modelling may reveal word relationships in our news articles, which can be used to extract themes or topics per article and for the entire corpus of news articles.\n",
    "\n",
    "We will test this hypothesis using two different popular topic models and our corpus of ~10,000 news articles from News-API\n",
    "\n",
    "### Research\n",
    "**1. LDA (Latent Dirichlet Allocation)**\n",
    "* generative probability model\n",
    "* discovers topics through probability distributions\n",
    "* produces probability weights per word\n",
    "<br/>[LDA and the ABC news headlines](https://medium.com/ml2vec/topic-modeling-is-an-unsupervised-learning-approach-to-clustering-documents-to-discover-topics-fdfbf30e27df)<br/><br/>\n",
    "\n",
    "**2. NMF (Non-negative Matrix Factorization)**\n",
    "* determanistic probability model\n",
    "* linear algebreic determination\n",
    "* produces probability weights per word\n",
    "<br/>[NMF and the New York Times](https://towardsdatascience.com/topic-modeling-for-the-new-york-times-news-dataset-1f643e15caac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "**We need a training set of articles for this experiment.**\n",
    "\n",
    "Let's load in ~1200 news headlines from Elastic and keep 80% for training and 20% for test. We'll focus on articles that mention *oil* or *gas*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. training articles: 559\n",
      "\n",
      "Sample description: It can be hard to get your head around just how much energy the world uses. Expressed in terms of oil, it was equivalent to almost 14 billion metric tons of the stuff in 2017. That’s like burning through all of Russia’s proved reserves in the space of 12 mont…\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch()\n",
    "\n",
    "docs = es.search(\n",
    "    index='articles', \n",
    "    doc_type='article',\n",
    "    q='description:\"oil\" OR description:\"gas\"',\n",
    "    filter_path=['hits.hits'],\n",
    "    _source_include='description,title',\n",
    "    sort='_id',\n",
    "    size=20000\n",
    ")\n",
    "\n",
    "articles = []\n",
    "\n",
    "for i,d in enumerate(docs['hits']['hits']):\n",
    "    desc = (d[\"_source\"][\"description\"])\n",
    "    if(desc):\n",
    "        articles.append(desc)\n",
    "    i+=1;\n",
    "    \n",
    "print(\"No. training articles:\",len(articles))\n",
    "print()\n",
    "print(\"Sample description:\",articles[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The sentences in the articles must be parsed to remove stop words, and split into individual words (tokenization). Then the words need to be encoded as floating point values to be used as input for our algorithms (vectorization).**\n",
    "\n",
    "Let's create some feature vectors for our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count Feature set shape: (559, 1000)\n",
      "TF-IDF Feature set shape: (559, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "n_features = 1000\n",
    "\n",
    "my_additional_stop_words = ['ap','said','says','time','monday','november']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_df=0.95, min_df=2, analyzer='word',\n",
    "                         max_features=n_features, stop_words=stop_words, token_pattern = r'\\b[a-zA-Z]{3,}\\b')\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=n_features, \n",
    "                        stop_words=stop_words)\n",
    "\n",
    "fs1_train = count_vectorizer.fit_transform(articles)\n",
    "fs2_train = tfidf_vectorizer.fit_transform(articles)\n",
    "\n",
    "print(\"Count Feature set shape:\",fs1_train.shape)\n",
    "print(\"TF-IDF Feature set shape:\",fs2_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next we will fit our topic models for both algorithms (LDA and NMF). We also need to specify the number of topics we are looking for. We can fine-tune this later but let's start by looking for 6 topics.**\n",
    "\n",
    "Let's generate our topic models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the LDA model...\n",
      "   done in: 0.6250646114349365\n",
      "\n",
      "Fitting the NMF model...\n",
      "   done in: 0.07584214210510254\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from time import time\n",
    "\n",
    "n_components = 6\n",
    "\n",
    "print ('Fitting the LDA model...')     \n",
    "t0 = time()\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "            learning_method='online', learning_offset=50., random_state=0)\n",
    "LDA = LDA.fit(fs1_train)\n",
    "\n",
    "print ('   done in:', (time()-t0))\n",
    "print ()\n",
    "\n",
    "print ('Fitting the NMF model...')\n",
    "t0 = time()\n",
    "\n",
    "NMF = NMF(n_components=n_components, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd')\n",
    "NMF = NMF.fit(fs2_train)\n",
    "\n",
    "print ('   done in:', (time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "**The most widely used method or evaluating a topic model is extrinsic evaluation, which means we manually evaluate the related words that the topic model generated to see if they make sense.**\n",
    "\n",
    "Let's have a look at the output of our two topic models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA - Topic model:\n",
      " Topic 0: flowing reimposes easing pressured strain supplies expectations dropped iranian angeles\n",
      " Topic 1: oil saudi arabia iran prices crude washington jamal khashoggi sanctions\n",
      " Topic 2: oil energy gas trump president company wednesday sanctions industry russian\n",
      " Topic 3: oil prices crude energy wednesday saudi supply markets inventories year\n",
      " Topic 4: oil energy china wall street officials journal producer refinery gas\n",
      " Topic 5: gas natural energy fracking pipeline company emissions oil greenhouse shale\n",
      "\n",
      "NMF - Topic model:\n",
      " Topic 0: iran sanctions month exports shortfall waivers replacing potential washington grant\n",
      " Topic 1: week lows expected stock sentiment slumped bearish markets inventories thursday\n",
      " Topic 2: gas energy natural company pipeline canadian corp columbia british liquefied\n",
      " Topic 3: prices oil supply crude demand rose journalist friday market disappearance\n",
      " Topic 4: arabia saudi struck russia output reuters high deal exporter private\n",
      " Topic 5: hurricane michael gulf mexico percent nearly 40 production florida offshore\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_terms = 10\n",
    "terms1 = count_vectorizer.get_feature_names()\n",
    "terms2 = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "print(\"LDA - Topic model:\")\n",
    "\n",
    "for topic_idx, topic in enumerate(LDA.components_):\n",
    "    message = \" Topic %d: \" % topic_idx\n",
    "    message += \" \".join([terms1[i]\n",
    "                    for i in topic.argsort()[:-n_terms - 1:-1]])\n",
    "    print(message)\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"NMF - Topic model:\")\n",
    "\n",
    "for topic_idx, topic in enumerate(NMF.components_):\n",
    "    message = \" Topic %d: \" % topic_idx\n",
    "    message += \" \".join([terms2[i]\n",
    "                    for i in topic.argsort()[:-n_terms - 1:-1]])\n",
    "    print(message)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "### 1. Topics are representative of current stories\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The topics generated by NMF seem to be more cohesive and make more sense than the LDA topics. We can see mention of events we know to be happening such as: Iranian sanctions, Hurricane Michael and Russia/Saudi discussing increasing production.**\n",
    "\n",
    "Let's have a closer look at the NMF topic model and some sample articles for each topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def display_topics(H, W, feature_names, documents, no_top_words, no_top_documents):\n",
    "    for topic_idx, topic in enumerate(H):\n",
    "        print(\"Topic %d\" % (topic_idx))\n",
    "        print(\"Keywords:\", \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        top_doc_indices = np.argsort( W[:,topic_idx] )[::-1][0:no_top_documents]\n",
    "        for doc_index in top_doc_indices:\n",
    "            print(\"Top document:\", documents[doc_index])\n",
    "            \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0\n",
      "Keywords: sanctions iran oil crude prices supply monday november iranian exports\n",
      "Top document: Brent crude oil prices fell more than 1 percent on Monday after Washington said it may grant waivers to sanctions against Iran's oil exports next month, and as Saudi Arabia was said to be replacing any potential shortfall from Iran.\n",
      "\n",
      "Topic 1\n",
      "Keywords: gas energy natural oil company pipeline new companies liquefied lng\n",
      "Top document: Canadian energy company TransCanada Corp told U.S. energy regulators on Thursday that the company's Columbia Gas Transmission unit put part of its $3 billion Mountaineer XPress natural gas pipeline into service in West Virginia.\n",
      "\n",
      "Topic 2\n",
      "Keywords: week inventories expected lows markets fell thursday bearish slumped investor\n",
      "Top document: Oil prices slumped to two-week lows on Thursday as global stock markets fell, with investor sentiment made more bearish by an industry report showing U.S. crude inventories rising more than expected.\n",
      "\n",
      "Topic 3\n",
      "Keywords: index canada main stock higher prices shares energy rose stocks\n",
      "Top document: Canada's main stock index opened higher on Monday as a rise in oil prices lifted shares of energy companies.\n",
      "\n",
      "Topic 4\n",
      "Keywords: saudi arabia output russia raise reuters opec high oil struck\n",
      "Top document: Oil traded below a four-year high on Wednesday as top exporter Saudi Arabia said it increased output to near a record high and after Reuters reported that Russia and Saudi Arabia had struck a private deal in September to pump more.\n",
      "\n",
      "Topic 5\n",
      "Keywords: gulf hurricane mexico michael percent platforms nearly tuesday production oil\n",
      "Top document: Nearly 40 percent of daily crude oil production was lost from offshore U.S. Gulf of Mexico wells on Tuesday due to platform evacuations and shut-ins ahead of Hurricane Michael.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nmf_W = NMF.transform(fs2_train)\n",
    "nmf_H = NMF.components_\n",
    "\n",
    "no_top_words = 10\n",
    "no_top_documents = 1\n",
    "display_topics(nmf_H, nmf_W, terms2, articles, no_top_words, no_top_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Number of topics is an important variable\n",
    "\n",
    "**Too few topics results in really broad general topics.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "n_components = 3\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "            learning_method='online', learning_offset=50., random_state=0)\n",
    "LDA = LDA.fit(fs1_train)\n",
    "    \n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.sklearn.prepare(LDA, fs1_train, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_components = 20\n",
    "\n",
    "LDA = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "            learning_method='online', learning_offset=50., random_state=0)\n",
    "LDA = LDA.fit(fs1_train)\n",
    "    \n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.sklearn.prepare(LDA, fs1_train, count_vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hypothesis: Topic modelling may reveal word relationships in our news articles, which can be used to extract themes or topics per article and for the entire corpus of news articles.*\n",
    "\n",
    "* Non-negative matrix factorization does a better job of clustering related words given a relatively small dataset (~1200 headlines), where as LDA struggled to provide cohesive results at this scale.\n",
    "<br/>\n",
    "\n",
    "* Each article could be classified by our topic model as we process new articles through the pipeline efficiently. This would provide a high-level (macro) category, but more granular tagging would require a different approach.\n",
    "<br/>\n",
    "\n",
    "* Difficult to determine the exact accuracy of the topic model, as the primary means of evaluation is extrinsic (human evaluation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Improvements\n",
    "\n",
    "1. Changing the initial query to Elastic had a significant impact on the topics produced. We could experiment with this query to dial in how broad or specific the topics are.\n",
    "<br/>\n",
    "\n",
    "2. The LDA model did not perform well with ~1200 articles but may perform better with a larger dataset - as generative models typically need lots of data.\n",
    "<br/>\n",
    "\n",
    "3. We may consider writing an additional component that labels the topics programatically. Example: query another source with the top 10 keywords and receive a high level topic label.\n",
    "<br/>\n",
    "\n",
    "4. Could add many more terms to the stop-words list to prevent topics from containing generic words such as 'tuesday'.\n",
    "<br/>\n",
    "\n",
    "5. Need to experiment with bi-grams and tri-gram vectorizers so that keyword phrases like 'british columbia' do not appear as seperate keywords 'british' and 'columbia'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
