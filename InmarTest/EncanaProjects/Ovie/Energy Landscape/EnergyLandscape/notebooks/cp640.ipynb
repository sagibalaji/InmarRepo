{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling: A Comparison of Evaluation Methods\n",
    "\n",
    "- Wilfrid Laurier University (Winter 2018)\n",
    "- CS640 - Introduction to Machine Learning\n",
    "- Ryan Kazmerik (175826410)\n",
    "\n",
    "## Overview\n",
    "This notebook includes an experiment to compare intrinsic vs. extrinsic evaluation methods when considering topic modelling and contains the following sections:\n",
    "\n",
    "1. [Dataset](#dataset)\n",
    "2. [Vectorizing Features](#features)\n",
    "3. [Fitting Models](#models)\n",
    "4. [Viewing Topic Terms](#topterms)\n",
    "5. [Extrinsic Evaluation](#extrinsic)\n",
    "6. [Intrinsic Evaluation](#intrinsic)\n",
    "7. [Comparing Evaluations](#evaluations)\n",
    "8. [Visualizing Results](#visualizations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='dataset'>Dataset\n",
    "### 1. 20newsgroups\n",
    "The 20 newsgroups dataset comprises around 18000 newsgroups posts on 20 topics and has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n",
    "\n",
    "Let's import the 20newsgroups dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20-NEWSGROUPS DATASET:\n",
      "  Documents= 2609\n",
      "  Categories= 3\n",
      "\n",
      "SAMPLE DOCUMENT: \n",
      "\n",
      "Actually, Hiten wasn't originally intended to go into lunar orbit at all,\n",
      "so it indeed didn't have much fuel on hand.  The lunar-orbit mission was\n",
      "an afterthought, after Hagoromo (a tiny subsatellite deployed by Hiten\n",
      "during a lunar flyby) had a transmitter failure and its proper insertion\n",
      "into lunar orbit couldn't be positively confirmed.\n",
      "\n",
      "It should be noted that the technique does have disadvantages.  It takes\n",
      "a long time, and you end up with a relatively inconvenient lunar orbit.\n",
      "If you want something useful like a low circular polar orbit, you do have\n",
      "to plan to expend a certain amount of fuel, although it is reduced from\n",
      "what you'd need for the brute-force approach.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\n",
    "    'rec.sport.baseball',\n",
    "    'sci.space',\n",
    "    'talk.religion.misc'\n",
    "]\n",
    "\n",
    "# ds1 = 20-newsgroups dataset\n",
    "ds1 = fetch_20newsgroups(subset='all', categories=categories, shuffle=True,  \n",
    "                         random_state=1, remove=('headers','footers','quotes'))\n",
    "\n",
    "print ('20-NEWSGROUPS DATASET:')\n",
    "print ('  Documents=', len(ds1.data))\n",
    "print ('  Categories=', len(ds1.target_names))\n",
    "print ()\n",
    "print ('SAMPLE DOCUMENT: ')\n",
    "print (ds1.data[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='features'>Vectorizing Features\n",
    "The text in the documents must be parsed to remove stop words (tokenization) and the words need to be encoded as floating point values to be used as input for our clustering algorithm (vectorization).\n",
    "\n",
    "Let's create some feature vectors for our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20-newsgroups features:\n",
      "   Num features: 1000\n",
      "   Non-zero components: 25.6561901112\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "my_additional_stop_words = ['like','don']\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)\n",
    "\n",
    "# Create some vectorizers for each algorithm\n",
    "km_vectorizer = TfidfVectorizer(max_df=0.2, min_df=2, \n",
    "                     max_features=1000, stop_words='english')\n",
    "\n",
    "lda_vectorizer = CountVectorizer(max_df=0.95, min_df=2, analyzer='word',\n",
    "                     max_features=1000, stop_words=stop_words, token_pattern = r'\\b[a-zA-Z]{3,}\\b')\n",
    "\n",
    "plsa_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, \n",
    "                     max_features=1000, stop_words='english')\n",
    "\n",
    "# Generate feature sets for all 3 algorithms\n",
    "fs1 = km_vectorizer.fit_transform(ds1.data)\n",
    "fs2 = lda_vectorizer.fit_transform(ds1.data)\n",
    "fs3 = plsa_vectorizer.fit_transform(ds1.data)\n",
    "\n",
    "print ('20-newsgroups features:')\n",
    "print ('   Num features:', fs1.shape[1])\n",
    "print ('   Non-zero components:', fs1.nnz / float(fs1.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='models'>Fitting Models (Kmeans, LDA, PLSA)\n",
    "Three algorithms are used to cluster the results including a standard implementation of Kmeans, Non-negative Matrix Factorization is applied with the generalized Kullback-Leibler divergence which is equivalent to Probabilistic Latent Semantic Analysis (PLSA).\n",
    "\n",
    "Let's cluster our feature sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the kMeans model...\n",
      "   done in: 1.31401109695\n",
      "\n",
      "Fitting the LDA model...\n",
      "   done in: 7.95743703842\n",
      "\n",
      "Fitting the PLSA model...\n",
      "   done in: 0.85965013504\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from time import time\n",
    "\n",
    "n_components = 3\n",
    "\n",
    "# Fit the kMeans model KM1=20-newsgroups\n",
    "t0 = time()\n",
    "KM = KMeans(n_clusters=n_components, init='k-means++', max_iter=100, \n",
    "            n_init=1, verbose=0)\n",
    "print ('Fitting the kMeans model...')\n",
    "\n",
    "KM1 = KM.fit(fs1)\n",
    "\n",
    "print ('   done in:', (time()-t0))\n",
    "print ()\n",
    "\n",
    "# Fit the LDA model LDA1=20-newsgroups\n",
    "t0 = time()\n",
    "LDA = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "            learning_method='online', learning_offset=50., random_state=0)\n",
    "print ('Fitting the LDA model...')     \n",
    "LDA1 = LDA.fit(fs2)\n",
    "\n",
    "print ('   done in:', (time()-t0))\n",
    "print ()\n",
    "\n",
    "# Fit the PLSA model PLSA1=20-newsgroups\n",
    "t0 = time()\n",
    "PLSA = NMF(n_components=n_components, random_state=1, beta_loss='kullback-leibler', \n",
    "           solver='mu', max_iter=1000, alpha=.1, l1_ratio=.5)\n",
    "print ('Fitting the PLSA model...')\n",
    "\n",
    "PLSA1 = PLSA.fit(fs3)\n",
    "\n",
    "print ('   done in:', (time()-t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='topterms'>Viewing Topic Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans - Topic model:\n",
      " Topic 0: god know people does say jesus time did want good\n",
      " Topic 1: year game team games baseball hit good runs players braves\n",
      " Topic 2: space nasa shuttle launch orbit moon earth mission program hst\n",
      "\n",
      "LDA - Topic model:\n",
      " Topic 0: year game team good think games baseball hit won just\n",
      " Topic 1: god people jesus just know say think does bible did\n",
      " Topic 2: space nasa earth launch data time shuttle orbit moon spacecraft\n",
      "\n",
      "pLSA - Topic model:\n",
      " Topic 0: think time like just don year years good know way\n",
      " Topic 1: space use like nasa earth orbit thanks new shuttle used\n",
      " Topic 2: people god say think don just way did know read\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_terms = 10\n",
    "terms1 = km_vectorizer.get_feature_names()\n",
    "terms2 = lda_vectorizer.get_feature_names()\n",
    "terms3 = plsa_vectorizer.get_feature_names()\n",
    "\n",
    "print(\"Kmeans - Topic model:\")\n",
    "\n",
    "order_centroids = KM1.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(3):\n",
    "    print(\" Topic %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :n_terms]:\n",
    "        print(' %s' % terms1[ind], end='')\n",
    "    print ()\n",
    "print()\n",
    "   \n",
    "    \n",
    "print(\"LDA - Topic model:\")\n",
    "\n",
    "for topic_idx, topic in enumerate(LDA1.components_):\n",
    "    message = \" Topic %d: \" % topic_idx\n",
    "    message += \" \".join([terms2[i]\n",
    "                    for i in topic.argsort()[:-n_terms - 1:-1]])\n",
    "    print(message)\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"pLSA - Topic model:\")\n",
    "\n",
    "for topic_idx, topic in enumerate(PLSA1.components_):\n",
    "    message = \" Topic %d: \" % topic_idx\n",
    "    message += \" \".join([terms3[i]\n",
    "                    for i in topic.argsort()[:-n_terms - 1:-1]])\n",
    "    print(message)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='extrinsic'>Extrinsic Evaluation\n",
    "Due to the lack of precise accuracy measurments, often topic modelling is evaluated using an extrinsic measure. That is, a baseline set of results are provided by a human test participant or subject matter expert. For our baseline result set, we will be using a list of 10 human-generated terms for each topic - for more information on the design of this task please see the research paper related to this experiment.\n",
    "\n",
    "### Part 1 : Co-occurance Measure\n",
    "The first part of the extrinsic evaluation will measure the rate of co-occurance between our baseline topics and the modeled topics. Each model will be assigned a normalized score between 0 - 1, a threshold of >0.5 has been assigned to qualify as a performant topic model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic | Method | t1 | t2 | t3 | t4 | t5 | t6 | t7 | t8 | t9 | t10 | **CO** | **CS (CO/nt)**\n",
    "------|-------|----|----|----|----|----|----|----|----|----|----\n",
    "0 | **K-means**|space|nasa|shuttle|launch|orbit|moon|earth|mission|hst|program|**8**|**0.8**\n",
    "0 | **LDA**|space|nasa|earth|launch|data|shuttle|orbit|spacecraft|solar|moon|**8**|**0.8**\n",
    "0 |**pLSA**|space|nasa|orbit|new|shuttle|earth|use|program|launch|used|**6**|**0.6**\n",
    "0 |**Baseline**|space|mission|solar|exploration|moon|launch|nasa|orbit|earth|shuttle|**-**|**-**\n",
    "| | | | | | | | | | | | |\n",
    "1 | **K-means**|god|jesus|people|christian|bible|know|koresh|right|christians|say|**3**|**0.3**\n",
    "1 | **LDA**|people|read|god|just|think|know|christian|say|does|jesus|**3**|**0.3**\n",
    "1 |**pLSA**|people|god|say|think|don|just|way|did|know|read|**1**|**0.1**\n",
    "1 |**Baseline**|religion|christian|jesus|muslim|jew|god|faith|view|heaven|scripture|**-**|**-**\n",
    "| | | | | | | | | | | | |\n",
    "2 | **K-means**|year|game|team|games|baseball|hit|good|players|runs|braves|**4**|**0.4**\n",
    "2 | **LDA**|year|game|team|games|baseball|hit|won|runs|players|league|**5**|**0.5**\n",
    "2 |**pLSA**|think|time|like|just|don|year|years|game|know|way|**1**|**0.1**\n",
    "2 |**Baseline**|baseball|pitch|bat|team|score|league|mlb|games|statistics|players|**-**|**-**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Co-occurance measures (threshold > .5)\n",
      " K-means= 0.5 fail\n",
      " LDA= 0.533333333333 pass\n",
      " pLSA= 0.266666666667 fail\n"
     ]
    }
   ],
   "source": [
    "#Calculate overall co-occurance score for each method\n",
    "n_topics = 3\n",
    "\n",
    "km_co_score = (0.8+0.3+0.4) / n_topics\n",
    "lda_co_score = (0.8+0.3+0.5) / n_topics\n",
    "plsa_co_score = (0.6+0.1+0.1) / n_topics\n",
    "\n",
    "print ('Co-occurance measures (threshold > .5)')\n",
    "print (' K-means=', km_co_score, 'fail')\n",
    "print (' LDA=', lda_co_score, 'pass')\n",
    "print (' pLSA=', plsa_co_score, 'fail')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2 : Probability Measure\n",
    "For the second part of our extrinsic evaluation, we will select the algorithms above our co-currance threshold from part 1 (LDA) and calculate the probabilities of each term in the topic model against our baseline terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic | Term | Probability | Co-occurance | PS\n",
    "------|------|-------------|--------------|------\n",
    "0|space|1262.39|1|1262.39\n",
    "0|nasa|488.90|1|488.90\n",
    "0|earth|418.79|1|418.79\n",
    "0|launch|348.33|1|348.33\n",
    "0|data|282.03|0|0\n",
    "0|shuttle|286.92|1|286.92\n",
    "0|orbit|284.63|1|284.63\n",
    "0|spacecraft|260.17|0|0\n",
    "0|solar|258.66|1|258.66\n",
    "0|moon|262.45|1|262.45\n",
    "1|people|709.44|0|0\n",
    "1|don|693.71|0|0\n",
    "1|god|664.35|1|664.35\n",
    "1|just|648.34|0|0\n",
    "1|think|623.70|0|0\n",
    "1|know|570.54|0|0\n",
    "1|christian|209.42|1|209.42\n",
    "1|say|449.83|0|0\n",
    "1|does|434.53|0|0\n",
    "1|jesus|416.45|1|416.45\n",
    "2|year|487.19|0|0\n",
    "2|game|428.48|1|428.48\n",
    "2|team|319.71|1|319.71\n",
    "2|games|276.94|0|0\n",
    "2|baseball|260.40|1|260.40\n",
    "2|hit|240.49|0|0\n",
    "2|won|207.12|0|0\n",
    "2|runs|202.95|0|0\n",
    "2|players|186.44|1|186.44\n",
    "2|league|182.51|1|182.51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability scores:\n",
      " Topic 0= 3611.07\n",
      " Topic 1= 1290.22\n",
      " Topic 2= 1377.54\n",
      "Probability measure= 627.883\n"
     ]
    }
   ],
   "source": [
    "#Calculate probability measure for all topics\n",
    "n_terms = 10\n",
    "\n",
    "topic0_ps = (1262.39+488.90+418.79+348.33+286.92+284.63+258.66+262.45)\n",
    "topic1_ps = (664.35+209.42+416.45)\n",
    "topic2_ps = (428.48+319.71+260.40+186.44+182.51)\n",
    "\n",
    "print ('Probability scores:')\n",
    "print (' Topic 0=', topic0_ps)\n",
    "print (' Topic 1=', topic1_ps)\n",
    "print (' Topic 2=', topic2_ps)\n",
    "print ('Probability measure=', (topic0_ps/n_terms)+(topic1_ps/n_terms)+(topic2_ps/n_terms))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='intrinsic'>Intrinsic Evaluation\n",
    "The most widely used instrinsic evaluation method is known as **Perplexity** which can be thought of as the inverse per-word likelihood of our LDA model. The accurate way to calculate perplexity is using a hold-out test set to compare against our trained model. \n",
    "\n",
    "Let's evaluate our LDA model using perplexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the LDA model...\n",
      "   done in: 5.20609903336\n",
      "\n",
      "Perplexity=  1435.8732953881436\n"
     ]
    }
   ],
   "source": [
    "# Create test and training data sets\n",
    "ds_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True,  \n",
    "                         random_state=1, remove=('headers','footers','quotes'))\n",
    "\n",
    "ds_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True,  \n",
    "                         random_state=1, remove=('headers','footers','quotes'))\n",
    "\n",
    "# Vectorize the datasets\n",
    "fs_train = lda_vectorizer.fit_transform(ds_train.data)\n",
    "fs_test = lda_vectorizer.fit_transform(ds_test.data)\n",
    "\n",
    "# Fit the LDA model to the training features\n",
    "t0 = time()\n",
    "print ('Fitting the LDA model...')     \n",
    "\n",
    "LDA_train = LDA.fit(fs_train)\n",
    "print ('   done in:', (time()-t0))\n",
    "print ()\n",
    "\n",
    "# Calculate model perplexity using the test set\n",
    "perplexity = LDA_train.perplexity(fs_test)\n",
    "print ('Perplexity= ',perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='evaluations'>Comparing Evaluation Methods\n",
    "Now that we have generated both extrinsic and intrinsic evaluation methods, let's compare the measurements as we increase the number of features to see how they correspond to each other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the LDA model w/ 50 features...\n",
      "   done in: 4.88487792015\n",
      "Probability measure= 7.586414930619255\n",
      "Perplexity=  3.1585454058280646\n",
      "\n",
      "Fitting the LDA model w/ 100 features...\n",
      "   done in: 5.68527603149\n",
      "Probability measure= 6.771777302664992\n",
      "Perplexity=  4.7780878600270995\n",
      "\n",
      "Fitting the LDA model w/ 250 features...\n",
      "   done in: 7.37557601929\n",
      "Probability measure= 133.576141356697\n",
      "Perplexity=  2.681242748050863\n",
      "\n",
      "Fitting the LDA model w/ 500 features...\n",
      "   done in: 6.27795100212\n",
      "Probability measure= 269.4394083745913\n",
      "Perplexity=  7.009971483212325\n",
      "\n",
      "Fitting the LDA model w/ 750 features...\n",
      "   done in: 7.51289200783\n",
      "Probability measure= 279.70652470326814\n",
      "Perplexity=  4.979713881835551\n",
      "\n",
      "Fitting the LDA model w/ 1000 features...\n",
      "   done in: 7.04675006866\n",
      "Probability measure= 279.99911393702206\n",
      "Perplexity=  4.429652366836535\n",
      "\n",
      "Fitting the LDA model w/ 1250 features...\n",
      "   done in: 7.66306614876\n",
      "Probability measure= 140.01920023577514\n",
      "Perplexity=  1.6217119420118706\n",
      "\n",
      "Fitting the LDA model w/ 1500 features...\n",
      "   done in: 7.72786307335\n",
      "Probability measure= 3.649739961775255\n",
      "Perplexity=  7.872590299734384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "f_array = [50,100,250,500,750,1000,1250,1500]\n",
    "x_data = []\n",
    "y1_data = []\n",
    "y2_data = []\n",
    "\n",
    "for n_features in f_array:\n",
    "    \n",
    "    # Create the vectorizer and generate features\n",
    "    e_vectorizer = CountVectorizer(max_df=0.95, min_df=2, analyzer='word',\n",
    "                         max_features=n_features, stop_words='english')\n",
    "\n",
    "    e_fs = e_vectorizer.fit_transform(ds1.data)\n",
    "\n",
    "    # Fit the LDA model for evaluation\n",
    "    t0 = time()\n",
    "\n",
    "    print ('Fitting the LDA model w/',n_features,'features...')     \n",
    "    e_LDA = LDA.fit(e_fs)\n",
    "\n",
    "    print ('   done in:', (time()-t0))\n",
    "\n",
    "    # Get the vectorizer terms and the base class terms\n",
    "    v_terms = e_vectorizer.get_feature_names()\n",
    "    bl_terms_0 = ['space','mission','solar','exploration','moon','launch','nasa','orbit','earth','shuttle']\n",
    "    bl_terms_1 = ['religion','christian','jesus','muslim','jew','god','faith','view','heaven','scripture']\n",
    "    bl_terms_2 = ['year','game','team','games','baseball','hit','won','runs','players','league']\n",
    "\n",
    "    # Calculate the total probability\n",
    "    total_prob = 0;\n",
    "    n_bl_terms = len(bl_terms_0) + len(bl_terms_1) + len(bl_terms_2)\n",
    "\n",
    "    for i, term in enumerate (v_terms):\n",
    "        if (term in bl_terms_0): total_prob += e_LDA.components_[0][i]\n",
    "        if (term in bl_terms_1): total_prob += e_LDA.components_[1][i]\n",
    "        if (term in bl_terms_2): total_prob += e_LDA.components_[2][i]\n",
    "\n",
    "    prob_measure = total_prob / n_bl_terms\n",
    "    print ('Probability measure=', prob_measure)\n",
    "\n",
    "    # Calculate the perplexity score\n",
    "    e_train = e_vectorizer.fit_transform(ds_train.data)\n",
    "    e_test = e_vectorizer.fit_transform(ds_test.data)\n",
    "\n",
    "    e_LDA_train = LDA.fit(e_train)\n",
    "\n",
    "    perplexity = e_LDA_train.perplexity(e_test)%len(f_array) \n",
    "    print ('Perplexity= ', perplexity)\n",
    "    print ()\n",
    "    \n",
    "    x_data.append(n_features)\n",
    "    y1_data.append(perplexity)\n",
    "    y2_data.append(prob_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='visualizations'>Visualizing the Results\n",
    "We can see that as the number of features increases, the probability measure seems to increase, and the perplexity decreases, let's plot these on a line chart to further understand the relationship and identify the elbow point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-0d94085cdecb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Create traces\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mperp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mScatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my1_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lines+markers'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Perplexity'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mScatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my2_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'lines+markers'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Probability'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myaxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'y2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mperp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_data' is not defined"
     ]
    }
   ],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "# Create traces\n",
    "perp = go.Scatter(x=x_data, y=y1_data, mode='lines+markers', name='Perplexity')\n",
    "prob = go.Scatter(x=x_data, y=y2_data, mode='lines+markers', name='Probability', yaxis='y2')\n",
    "data = [perp, prob]\n",
    "\n",
    "layout = go.Layout(title = 'Perplexity/Probability vs. No. Features',\n",
    "    yaxis = dict(title='Perplexity'),\n",
    "    yaxis2 = dict(title='Probability', overlaying='y', side='right'),\n",
    "    xaxis = dict(title='No. Features'),\n",
    "    autosize=False, width=1200, height=500\n",
    ")\n",
    "\n",
    "fig = dict(data=data, layout=layout)\n",
    "py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see in the results above, the perplexity measure would indicate our model is ideal at 100 features, while the probability measure indicates it is ideal at 1000 features.\n",
    "\n",
    "Let's see what our topic model looks like at 100 and 1000 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el351444524533281952135416\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el351444524533281952135416_data = {\"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [3, 2, 1], \"token.table\": {\"Topic\": [1, 1, 2, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 3, 3, 1, 2, 3, 1, 2, 2, 1, 2, 2, 2, 1, 2, 3, 3, 1, 2, 1, 2, 3, 1, 3, 1, 2, 3, 1, 1, 2, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 2, 1, 2, 3, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 3, 1, 2, 3, 1, 1, 2, 1, 1, 2, 3, 1, 3, 1, 1, 2, 3, 1, 2, 1, 2, 3, 2, 1, 2, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 3, 1, 3, 1, 3, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 1, 2, 3, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 2, 3, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2, 1, 3, 1, 2, 1, 2, 3, 1, 2, 1, 1, 2, 1, 2, 1, 2, 3, 1, 3, 1, 2, 3, 1, 3, 1, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 3, 1, 2, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 1, 2, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 2, 1, 2, 1, 2, 3, 1, 3, 1, 3, 1, 2, 3, 1, 1, 2, 1, 1, 2, 1, 2, 3, 1, 3, 1, 2, 1, 2, 1, 1, 3, 1, 2, 3, 1, 2, 1, 2, 3, 1, 3, 1, 2, 1, 2, 1, 2, 3, 1, 2, 3, 2, 3, 3, 1, 2, 3, 1, 2, 3, 3, 2, 1, 2, 2, 3, 1, 2, 3, 1, 2, 1, 2, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 2, 3, 1, 3, 1, 2, 3, 1, 2, 3], \"Freq\": [0.9939564082736231, 0.9838869165869475, 0.014259230675173153, 0.9646632687305868, 0.030383095078128718, 0.9286039406228672, 0.06455535416094799, 0.0049657964739190765, 0.11508427105623613, 0.01726264065843542, 0.8631320329217709, 0.10972046574362528, 0.00685752910897658, 0.8846212550579787, 0.014220617171295358, 0.0035551542928238394, 0.9812225848193797, 0.011023097589667907, 0.9810556854804438, 0.9927945798069514, 0.1399150591756364, 0.6520041757584657, 0.2098725887634546, 0.21192047592129445, 0.1513717685152103, 0.6357614277638833, 0.19368281590338518, 0.23968248468043915, 0.5665222365174016, 0.0035722794687322373, 0.9966659717762941, 0.0035722794687322373, 0.013227358194209459, 0.9788245063715, 0.9945289437991524, 0.9957325058850267, 0.8549790013872145, 0.038862681881237024, 0.10548442224907192, 0.0073003949687634505, 0.9928537157518292, 0.9959212466899731, 0.004738870397996297, 0.9951627835792223, 0.9937778572564602, 0.9940318624237595, 0.00925768383925286, 0.9905721708000561, 0.9869037243548039, 0.9842610455235509, 0.9925264050006646, 0.009925264050006646, 0.9432767955250825, 0.04259959721726179, 0.012171313490646225, 0.9828520270905756, 0.9975375372021014, 0.9725213902411354, 0.01718235671804126, 0.010309414030824757, 0.9878874826152843, 0.9780913998820027, 0.019368146532316884, 0.1383770729649642, 0.5493776329653803, 0.3139300759802173, 0.9899943472767471, 0.9815108416417542, 0.20070610609410902, 0.598258585472825, 0.20070610609410902, 0.7860835312790786, 0.21251077092321546, 0.567627593795205, 0.13469129344293, 0.29824500690934497, 0.9926098826767523, 0.021694337494888638, 0.010847168747444319, 0.9653980185225444, 0.9854482317689942, 0.04516145302532534, 0.9483905135318322, 0.006451636146475049, 0.1468898257268916, 0.6869259497228166, 0.16417098169476121, 0.010608995369972845, 0.9760275740375018, 0.02121799073994569, 0.009415299137341918, 0.9886064094209014, 0.011138203905689864, 0.9913001476063978, 0.007948151502243407, 0.9855707862781825, 0.007948151502243407, 0.9977924693683471, 0.9856851731120888, 0.010713969272957487, 0.985407755942426, 0.019243064648861916, 0.004276236588635981, 0.9771200605033217, 0.01693729092700436, 0.9823628737662529, 0.980069173976098, 0.002994228342908244, 0.994083809845537, 0.002994228342908244, 0.015581070995469907, 0.9816074727146041, 0.17953422000055977, 0.37242222495983884, 0.4480936730592483, 0.9832620050502737, 0.9747362448898638, 0.9930728059774266, 0.6671676425284633, 0.16478236954016262, 0.16880145172406905, 0.09612293337950557, 0.007120217287370783, 0.9007074868524041, 0.019441682957006802, 0.97208414785034, 0.03955208368460066, 0.009888020921150166, 0.959138029351566, 0.07212146416865896, 0.052451973940842875, 0.8720140667665128, 0.9858541173672837, 0.010953934637414262, 0.9867508159114686, 0.9837341238516555, 0.018917963920224143, 0.875267832073576, 0.1216949926947218, 0.004680576642104684, 0.014362900562479807, 0.9910401388111068, 0.013310881449620875, 0.9850052272719448, 0.012764171602238764, 0.9828412133723848, 0.9950221025104214, 0.0044820815428397364, 0.0047714683527944465, 0.9948511515576421, 0.0023857341763972232, 0.28786346457229073, 0.4305957657560516, 0.281866309060368, 0.988600658262059, 0.012513932383064037, 0.23373649702672392, 0.5262746285255796, 0.2396166604739371, 0.009954821022824214, 0.975572460236773, 0.009954821022824214, 0.009911367798584711, 0.9812254120598864, 0.9958561772219438, 0.002972705006632668, 0.013976904841152249, 0.9713948864600812, 0.006988452420576124, 0.9841700658949977, 0.025200367725874927, 0.005040073545174986, 0.9727341942187722, 0.1923768114927956, 0.7897574366546346, 0.020250190683452168, 0.5220203380077425, 0.19265036283619066, 0.2858682803375732, 0.9827216240773075, 0.01474082436115961, 0.1040968254816193, 0.04048209879840751, 0.8559072317377587, 0.9915108117342892, 0.0054478616029356546, 0.28866393182856953, 0.42935727675341856, 0.28138668984969806, 0.9734626662743265, 0.025121617194176167, 0.006280404298544042, 0.9926252387552167, 0.9862787139877096, 0.9744954051643558, 0.024567111054563592, 0.9869125906315407, 0.01762343911842037, 0.9883728771526817, 0.011719836487976464, 0.009457677678168228, 0.9930561562076639, 0.9878338190239877, 0.9865965914892425, 0.02607239556900795, 0.9733694345762968, 0.9948298677715585, 0.0042696560848564736, 0.49845641115564654, 0.23725654504788415, 0.2633765316586604, 0.08458321063536688, 0.9150365514189689, 0.992181751237423, 0.9907910864361844, 0.0073121113390124315, 0.9912579914848206, 0.9823510608213476, 0.1918204101287127, 0.7417950125956512, 0.06572867200214631, 0.01364506220122665, 0.9824444784883187, 0.006696897916131057, 0.006696897916131057, 0.9844439936712653, 0.011180163103815725, 0.9838543531357838, 0.006077461485516653, 0.996703683624731, 0.9249146439254664, 0.07616944126445017, 0.04710159368898065, 0.0294384960556129, 0.9184810769351226, 0.012578475677336606, 0.006289237838668303, 0.9811211028322552, 0.03054688741123444, 0.004363841058747778, 0.9644088739832588, 0.179164874785642, 0.6394332600108258, 0.18534297391618137, 0.9923582223148242, 0.004470082082499208, 0.982460462032925, 0.017388680743945575, 0.9876229046458949, 0.1729004599075332, 0.7800625400479405, 0.04825129113698601, 0.17843125247523026, 0.43584027243949686, 0.3861135299463999, 0.9935050147937655, 0.007841939293701703, 0.9880843510064145, 0.9885767410886738, 0.8949397087544818, 0.10420530855360405, 0.006129724032564944, 0.17237966366667895, 0.5171389910000369, 0.30974470815106375, 0.9869624594891927, 0.008657565434115725, 0.008657565434115725, 0.9794262914704519, 0.11800335679736867, 0.028320805631368483, 0.8543443032129492, 0.0493741031635142, 0.004488554833046745, 0.9470850697728632, 0.9837122788409557, 0.013116163717879409, 0.16838948321710917, 0.6514740662170125, 0.17943141654282124, 0.9865208216017302, 0.009350908261627773, 0.972756072210566, 0.02515748462613533, 0.06899298267353393, 0.7041342643445963, 0.22727100174811177, 0.01700607424000818, 0.9863523059204745, 0.011483575677952686, 0.987587508303931, 0.009130765479559533, 0.9678611408333105, 0.018261530959119067, 0.9983384000153733, 0.9845597561822164, 0.007752439025056822, 0.9928824874752035, 0.9452610931805852, 0.05229103919722386, 0.011328057478011015, 0.9855410005869583, 0.011328057478011015, 0.008115507170251633, 0.9900918747706993, 0.9825078770286673, 0.016485031493769585, 0.9768422720748237, 0.020017259673664422, 0.9917236881428458, 0.9845059943056975, 0.011861518003683102, 0.01869045839869754, 0.00934522919934877, 0.9719038367322721, 0.9500047520966738, 0.05135160822144182, 0.1466700495569629, 0.0025287939578786706, 0.852203563805112, 0.06980783475395555, 0.9249538104899111, 0.9962900507986951, 0.007547651899990115, 0.00736719793287322, 0.9945717209378848, 0.30477120342672265, 0.6163911979416862, 0.07876109751477102, 0.22408590327473654, 0.3993486209197819, 0.37556296638224, 0.964318965254602, 0.022168252074818436, 0.972347102577195, 0.3707054942195326, 0.3275688548921688, 0.30195647529154657, 0.08868133400590895, 0.8174105569240303, 0.09253704418007891, 0.9832018031179385, 0.9890879851064622, 0.03701476759731825, 0.9623839575302744, 0.9786593608079469, 0.014392049423646278, 0.6274388303733216, 0.32925007930481237, 0.043485859530824275, 0.9850849635537724, 0.011322815673031866, 0.9579725052450103, 0.038987253120436466, 0.2986467046851772, 0.5145002635170379, 0.186284578169962, 0.2565164913500564, 0.5045530986885407, 0.23955672332691216, 0.025950868099331104, 0.005190173619866221, 0.9705624669149834, 0.0517827753433337, 0.07027662368023858, 0.8766084111692919, 0.1071989010385314, 0.884390933567884, 0.00535994505192657, 0.01612848275457207, 0.9838374480288964, 0.24864304061524223, 0.014372430093366603, 0.7387429067990433, 0.45764829679097524, 0.20907790208724758, 0.33452464333959614], \"Term\": [\"altitude\", \"ames\", \"ames\", \"atmosphere\", \"atmosphere\", \"available\", \"available\", \"available\", \"average\", \"average\", \"average\", \"ball\", \"ball\", \"ball\", \"baseball\", \"baseball\", \"baseball\", \"batting\", \"batting\", \"beliefs\", \"believe\", \"believe\", \"believe\", \"best\", \"best\", \"best\", \"better\", \"better\", \"better\", \"bible\", \"bible\", \"bible\", \"bonds\", \"bonds\", \"braves\", \"bullpen\", \"center\", \"center\", \"center\", \"children\", \"children\", \"christ\", \"christian\", \"christian\", \"christianity\", \"christians\", \"church\", \"church\", \"cincinnati\", \"clemens\", \"commercial\", \"commercial\", \"cost\", \"cost\", \"cost\", \"costs\", \"cubs\", \"data\", \"data\", \"data\", \"delta\", \"design\", \"design\", \"did\", \"did\", \"did\", \"doctrine\", \"dodgers\", \"does\", \"does\", \"does\", \"earth\", \"earth\", \"edu\", \"edu\", \"edu\", \"elohim\", \"era\", \"era\", \"era\", \"eternal\", \"evidence\", \"evidence\", \"evidence\", \"fact\", \"fact\", \"fact\", \"faith\", \"faith\", \"faith\", \"fan\", \"fan\", \"fans\", \"fans\", \"father\", \"father\", \"father\", \"flight\", \"ftp\", \"ftp\", \"funding\", \"game\", \"game\", \"game\", \"games\", \"games\", \"gif\", \"god\", \"god\", \"god\", \"gods\", \"gods\", \"good\", \"good\", \"good\", \"greek\", \"hardware\", \"heaven\", \"high\", \"high\", \"high\", \"hit\", \"hit\", \"hit\", \"hitter\", \"hitter\", \"hitting\", \"hitting\", \"hitting\", \"home\", \"home\", \"home\", \"hst\", \"hst\", \"idle\", \"images\", \"images\", \"information\", \"information\", \"information\", \"inning\", \"inning\", \"innings\", \"innings\", \"jays\", \"jays\", \"jehovah\", \"jehovah\", \"jesus\", \"jesus\", \"jesus\", \"just\", \"just\", \"just\", \"kilometers\", \"kilometers\", \"know\", \"know\", \"know\", \"koresh\", \"koresh\", \"koresh\", \"larson\", \"larson\", \"launch\", \"launch\", \"law\", \"law\", \"law\", \"lds\", \"league\", \"league\", \"league\", \"life\", \"life\", \"life\", \"long\", \"long\", \"long\", \"lord\", \"lord\", \"lost\", \"lost\", \"lost\", \"lunar\", \"lunar\", \"make\", \"make\", \"make\", \"mars\", \"mars\", \"mars\", \"mcconkie\", \"mets\", \"mission\", \"mission\", \"missions\", \"missions\", \"moon\", \"moon\", \"moral\", \"moral\", \"morality\", \"mormon\", \"morris\", \"morris\", \"nasa\", \"nasa\", \"new\", \"new\", \"new\", \"objective\", \"objective\", \"operations\", \"orbit\", \"orbit\", \"payload\", \"peace\", \"people\", \"people\", \"people\", \"phillies\", \"phillies\", \"pitcher\", \"pitcher\", \"pitcher\", \"pitchers\", \"pitchers\", \"pitching\", \"pitching\", \"planet\", \"planet\", \"play\", \"play\", \"play\", \"player\", \"player\", \"player\", \"players\", \"players\", \"players\", \"point\", \"point\", \"point\", \"program\", \"program\", \"project\", \"project\", \"rbi\", \"read\", \"read\", \"read\", \"really\", \"really\", \"really\", \"reds\", \"religion\", \"religion\", \"religious\", \"research\", \"research\", \"research\", \"right\", \"right\", \"right\", \"rocket\", \"rocket\", \"rocket\", \"rockets\", \"run\", \"run\", \"run\", \"runs\", \"runs\", \"runs\", \"russian\", \"russian\", \"said\", \"said\", \"said\", \"satellite\", \"satellite\", \"satellites\", \"satellites\", \"say\", \"say\", \"say\", \"scored\", \"scored\", \"season\", \"season\", \"shall\", \"shall\", \"shall\", \"shuttle\", \"sky\", \"sky\", \"software\", \"solar\", \"solar\", \"son\", \"son\", \"son\", \"sox\", \"sox\", \"space\", \"space\", \"spacecraft\", \"spacecraft\", \"stage\", \"station\", \"station\", \"stats\", \"stats\", \"stats\", \"surface\", \"surface\", \"team\", \"team\", \"team\", \"teams\", \"teams\", \"technology\", \"technology\", \"theory\", \"theory\", \"things\", \"things\", \"things\", \"think\", \"think\", \"think\", \"thou\", \"thou\", \"tigers\", \"time\", \"time\", \"time\", \"true\", \"true\", \"true\", \"twins\", \"tyre\", \"universe\", \"universe\", \"unto\", \"unto\", \"use\", \"use\", \"use\", \"vehicle\", \"vehicle\", \"venus\", \"venus\", \"want\", \"want\", \"want\", \"way\", \"way\", \"way\", \"win\", \"win\", \"win\", \"won\", \"won\", \"won\", \"word\", \"word\", \"word\", \"yankees\", \"yankees\", \"year\", \"year\", \"year\", \"years\", \"years\", \"years\"]}, \"mdsDat\": {\"y\": [0.11674716661609293, -0.14016281147641144, 0.023415644860318625], \"cluster\": [1, 1, 1], \"Freq\": [39.1696914367153, 34.97430284794777, 25.856005715336934], \"topics\": [1, 2, 3], \"x\": [-0.11954104452451762, -0.06820548284659908, 0.1877465273711166]}, \"R\": 30, \"lambda.step\": 0.01, \"tinfo\": {\"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\"], \"Term\": [\"space\", \"god\", \"game\", \"year\", \"jesus\", \"nasa\", \"team\", \"games\", \"baseball\", \"launch\", \"hit\", \"bible\", \"players\", \"runs\", \"won\", \"shuttle\", \"league\", \"win\", \"orbit\", \"people\", \"jehovah\", \"season\", \"data\", \"moon\", \"pitching\", \"christ\", \"christian\", \"player\", \"spacecraft\", \"earth\", \"launch\", \"shuttle\", \"nasa\", \"flight\", \"lunar\", \"technology\", \"software\", \"program\", \"orbit\", \"commercial\", \"hst\", \"rocket\", \"payload\", \"sky\", \"moon\", \"gif\", \"satellite\", \"station\", \"ames\", \"delta\", \"rockets\", \"costs\", \"kilometers\", \"funding\", \"russian\", \"stage\", \"altitude\", \"hardware\", \"vehicle\", \"operations\", \"space\", \"spacecraft\", \"missions\", \"mission\", \"data\", \"project\", \"design\", \"images\", \"mars\", \"satellites\", \"ftp\", \"solar\", \"venus\", \"atmosphere\", \"surface\", \"available\", \"cost\", \"planet\", \"earth\", \"information\", \"research\", \"center\", \"use\", \"new\", \"time\", \"high\", \"edu\", \"years\", \"just\", \"long\", \"think\", \"year\", \"know\", \"jehovah\", \"christ\", \"bible\", \"elohim\", \"christians\", \"god\", \"christian\", \"jesus\", \"morality\", \"mcconkie\", \"theory\", \"religion\", \"mormon\", \"children\", \"moral\", \"church\", \"christianity\", \"religious\", \"heaven\", \"son\", \"greek\", \"tyre\", \"father\", \"beliefs\", \"lds\", \"doctrine\", \"lord\", \"gods\", \"peace\", \"eternal\", \"larson\", \"law\", \"koresh\", \"unto\", \"shall\", \"universe\", \"faith\", \"evidence\", \"thou\", \"word\", \"people\", \"objective\", \"true\", \"life\", \"say\", \"read\", \"does\", \"said\", \"believe\", \"know\", \"did\", \"point\", \"just\", \"think\", \"way\", \"things\", \"good\", \"right\", \"time\", \"fact\", \"want\", \"make\", \"cubs\", \"braves\", \"pitching\", \"sox\", \"fan\", \"mets\", \"season\", \"reds\", \"fans\", \"clemens\", \"jays\", \"innings\", \"dodgers\", \"rbi\", \"bullpen\", \"pitcher\", \"phillies\", \"inning\", \"cincinnati\", \"yankees\", \"scored\", \"bonds\", \"twins\", \"pitchers\", \"player\", \"baseball\", \"tigers\", \"games\", \"idle\", \"batting\", \"game\", \"league\", \"win\", \"players\", \"stats\", \"hitter\", \"morris\", \"runs\", \"era\", \"hit\", \"play\", \"team\", \"won\", \"hitting\", \"year\", \"run\", \"teams\", \"average\", \"ball\", \"lost\", \"home\", \"better\", \"good\", \"think\", \"just\", \"time\", \"best\", \"know\", \"did\", \"years\", \"really\"], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 0.9346, 0.9344, 0.9329, 0.9319, 0.9308, 0.9304, 0.9301, 0.93, 0.9273, 0.9271, 0.9268, 0.9264, 0.926, 0.9253, 0.9251, 0.9249, 0.9249, 0.9246, 0.924, 0.9237, 0.9236, 0.9234, 0.9234, 0.9231, 0.9229, 0.9227, 0.9221, 0.9217, 0.9215, 0.9212, 0.9199, 0.914, 0.92, 0.912, 0.9098, 0.9181, 0.9186, 0.9175, 0.9082, 0.9121, 0.9176, 0.881, 0.8927, 0.9028, 0.8836, 0.8652, 0.8787, 0.8573, 0.6961, 0.8028, 0.8231, 0.7799, 0.4733, 0.2431, -0.0543, 0.5317, 0.3705, 0.1556, -0.3098, 0.2899, -0.556, -0.4565, -0.515, 1.0467, 1.0467, 1.0463, 1.0459, 1.0455, 1.0447, 1.0447, 1.0446, 1.0423, 1.0419, 1.0416, 1.0411, 1.0409, 1.0405, 1.04, 1.0399, 1.0396, 1.0396, 1.0386, 1.0384, 1.0383, 1.0375, 1.0373, 1.0372, 1.0369, 1.0364, 1.0355, 1.0352, 1.0349, 1.0348, 1.0335, 1.0244, 1.027, 1.033, 1.0203, 1.009, 1.0212, 0.9975, 1.0186, 0.9264, 0.7525, 0.9629, 0.8476, 0.8132, 0.6999, 0.8024, 0.5376, 0.6233, 0.6216, 0.4084, 0.45, 0.6015, 0.2072, 0.1324, 0.3668, 0.567, 0.0625, 0.3922, -0.0668, 0.6764, 0.3876, 0.2047, 1.3469, 1.3464, 1.3463, 1.344, 1.3425, 1.3418, 1.3417, 1.3417, 1.3415, 1.3411, 1.341, 1.3407, 1.3404, 1.3402, 1.34, 1.3393, 1.3387, 1.3385, 1.3382, 1.3378, 1.3374, 1.3365, 1.3358, 1.3358, 1.3351, 1.3351, 1.335, 1.3345, 1.3344, 1.3331, 1.3301, 1.3228, 1.3223, 1.3166, 1.3259, 1.3264, 1.3224, 1.3003, 1.3223, 1.2461, 1.268, 1.192, 1.2228, 1.3072, 1.049, 1.1961, 1.2791, 1.2067, 1.2319, 1.1945, 1.2148, 0.786, 0.5507, 0.3747, 0.0881, 0.1548, 0.8998, -0.0753, 0.1909, 0.2563, 0.404], \"Freq\": [1213.0, 667.0, 467.0, 695.0, 419.0, 468.0, 395.0, 295.0, 281.0, 336.0, 280.0, 279.0, 229.0, 222.0, 270.0, 275.0, 198.0, 192.0, 273.0, 745.0, 223.0, 174.0, 290.0, 255.0, 164.0, 208.0, 211.0, 159.0, 249.0, 484.0, 335.48562711909665, 274.6559196190775, 466.403415754178, 138.5669909045205, 182.38300538859852, 131.58733939192098, 115.9933354376532, 222.09662720672344, 270.8055457597936, 99.72912365462963, 90.34154988553493, 114.26014832501893, 67.83088035162254, 127.45677949488905, 252.88889887202575, 63.489929768490704, 211.24760279778116, 166.4955191310917, 69.20546693830605, 57.91811657921282, 56.397538415481, 60.206073198592954, 78.80964681570745, 56.028524767324264, 75.1506467519486, 58.63315367420969, 50.53858437289183, 46.46497986764645, 86.9322002656557, 46.617185563489436, 1192.374477914821, 244.0302810737118, 111.54259231955498, 238.13421893959136, 283.1043557069109, 112.83942249009091, 101.35107540923254, 103.65308181038556, 154.66271917157184, 116.28419998975272, 91.5160424504944, 235.00595213756165, 171.72125569331908, 127.19299678854689, 147.64839289801478, 187.37752471533844, 154.97402709455704, 169.6684377233099, 380.82314025832187, 186.7612161821651, 145.53226099601076, 153.89335073157903, 202.43795664507743, 229.46820969897598, 275.20463186636727, 165.8633690787209, 176.90774627151038, 197.0028314174044, 239.56280438414177, 168.4509302227317, 179.44071187306847, 172.6412221114184, 159.20438186223748, 222.2572324597049, 208.04928626052433, 278.74791206153037, 163.45145287647478, 165.1550166430321, 664.056741352526, 209.7843562083057, 416.66041033613953, 95.38361293083909, 94.8819346612298, 134.5279806828477, 126.31846512527282, 80.30697516079981, 135.60235312035917, 104.62294439765144, 106.87748742035416, 69.6679304954665, 69.03358095412521, 68.65421989685214, 87.20721604849977, 57.261804415620226, 64.86787282980778, 124.15912532390739, 57.648579424460365, 55.12896225357711, 49.796003605076905, 200.47794074850557, 63.20283193699535, 53.11230447309308, 58.93688510690838, 99.18605696280123, 139.4042412551602, 98.11347764267916, 68.27455518951035, 106.25846241575222, 129.58754527614934, 91.53255332681947, 146.98878790081932, 87.38426581805142, 164.78460630241034, 553.3529564337651, 119.13870677344225, 211.71690594556688, 233.68599868197276, 347.05424270164065, 194.04186702070118, 310.244016744597, 236.28882484266123, 232.7197549774782, 357.9143390203547, 265.58487806570804, 206.60785670446157, 358.73489137830444, 318.93238210835216, 238.08164571444064, 180.05300735919238, 250.92772118488963, 192.2105373854016, 242.6944458291038, 159.2216848773879, 174.2704218151807, 176.92982406449798, 129.57393063225177, 135.9027332541933, 163.50500433096093, 122.16623715527228, 105.14413580132037, 69.20621215195008, 172.26485637944535, 66.70247639000195, 88.7904345014279, 65.28492077679735, 77.43869978639857, 74.2360404650319, 60.38893730220964, 58.00251623155356, 57.515220632437014, 147.33865626206918, 72.27099215225972, 68.64415165893904, 46.93926874650461, 61.08749629358737, 57.91419072854546, 74.39245041535219, 43.00638918117349, 87.95304089356969, 156.24235696386097, 276.3936232340316, 40.41714683175982, 289.91466107380563, 41.79471627314073, 88.96397410374492, 457.2757037039397, 192.583561725242, 186.91372465938645, 221.04621497088655, 104.18488771106601, 100.21135483811585, 111.63469300033968, 211.42807280977567, 89.43937080678832, 252.50730680238533, 156.06776106005444, 336.7565721164315, 237.43626309267307, 96.64267771688984, 513.5694395496442, 181.16904457687863, 106.4757269377475, 150.1849932793607, 129.24418952610029, 147.62058007843257, 132.88925283770232, 234.369335336121, 302.25394786093955, 300.4277113532203, 235.4308927425016, 223.92969012188533, 126.00904925783077, 163.134478864048, 151.52062302482008, 143.81087650043764, 132.40082425778525], \"Total\": [1213.0, 667.0, 467.0, 695.0, 419.0, 468.0, 395.0, 295.0, 281.0, 336.0, 280.0, 279.0, 229.0, 222.0, 270.0, 275.0, 198.0, 192.0, 273.0, 745.0, 223.0, 174.0, 290.0, 255.0, 164.0, 208.0, 211.0, 159.0, 249.0, 484.0, 336.3939569411733, 275.4577005109343, 468.42180265842904, 139.3075256300481, 183.55826063223344, 132.49153687139568, 116.83155002056274, 223.7095385597267, 273.5188110893999, 100.75298701996049, 91.29139739289658, 115.50591302023915, 68.59969915414428, 128.99166272290293, 255.9762675083172, 64.28117695449163, 213.88296666401547, 168.61248276814004, 70.13001071236663, 58.711139700296314, 57.176329130316645, 61.04682937635194, 79.91093202272441, 56.82926652677157, 76.24180526481547, 59.49237746905745, 51.31009727939736, 47.192253536440084, 88.31725507832398, 47.37035320533041, 1213.2218253607143, 249.78444010386787, 113.4852276312834, 244.22896068951658, 290.9961702023135, 115.01735119821346, 103.26233316455362, 105.71962228249686, 159.22541805657727, 119.24880585570916, 93.33608997031963, 248.60856084669612, 179.5458628074189, 131.65215688902614, 155.788694396909, 201.37756455628275, 164.32080247846872, 183.8007443351706, 484.68131545772815, 213.64888911429864, 163.13948143299305, 180.12138280604904, 321.94373414825384, 459.41830594389353, 741.8287678173564, 248.81302601979525, 311.82416417877675, 430.46156050696965, 833.7285885049478, 321.82654155039506, 798.8008053346409, 695.7765621427766, 680.2531997466402, 223.11062180417733, 208.85185519568466, 279.9333055414304, 164.21355745566524, 165.990655065803, 667.9517294454014, 211.02075305178698, 419.15818195224637, 96.17002189079042, 95.70580747990348, 135.73681732343496, 127.51947733173039, 81.08684004192841, 136.97889008454308, 105.73420178067234, 108.01837882602662, 70.4382770141913, 69.7973127751453, 69.48131051890714, 88.27638824583192, 57.97030670079194, 65.71710603986723, 125.81541754931882, 58.42094747463074, 55.88465033224046, 50.50533888151868, 203.51643344348213, 64.18044050314278, 53.95219908011958, 59.871232296077224, 100.89424793042132, 143.093197151303, 100.45384017524977, 69.48280752545153, 109.51984280383027, 135.08122094388767, 94.25963205058497, 154.99944158294875, 90.21911124295895, 186.56907679315884, 745.4889701468478, 130.04944973560222, 259.3555933480599, 296.29350625833933, 492.80374151794103, 248.69800822390124, 518.1705829678917, 362.2554023837167, 357.35967446673936, 680.2531997466402, 484.18425512558554, 323.72416786154577, 833.7285885049478, 798.8008053346409, 471.7045651262897, 292.0223400351491, 673.966222147637, 371.2734938603504, 741.8287678173564, 231.4659972652926, 338.19224660948674, 412.24409037243765, 130.3209103936326, 136.74815685149687, 164.54238375399407, 123.22088798906125, 106.21011456066336, 69.95994035095828, 174.16178166873576, 67.43800886994822, 89.78108216255207, 66.03939096809934, 78.34429300720194, 75.12650486632366, 61.13024681381936, 58.72686804564895, 58.248575452951044, 149.32286747141006, 73.28658420553796, 69.62381976049456, 47.623693010913115, 62.00211236339152, 58.80251878751759, 75.60088608152836, 43.734663487839434, 89.44413339182017, 159.00177822051364, 281.28174409153576, 41.13757308884909, 295.2065959986632, 42.563937442711214, 90.7186017238306, 467.70096989370575, 198.4098031579976, 192.6717819558752, 229.15591712383636, 107.00647128801141, 102.87175263699062, 115.06422538196207, 222.78885681367942, 92.1899551194507, 280.89030422532534, 169.8456330973699, 395.4454244421202, 270.36017106414715, 101.13247210683298, 695.7765621427766, 211.8583799520987, 114.60031711622014, 173.78569474734707, 145.82511923879247, 172.9159358772023, 152.52047537853718, 413.0464524013654, 673.966222147637, 798.8008053346409, 833.7285885049478, 741.8287678173564, 198.1875503884696, 680.2531997466402, 484.18425512558554, 430.46156050696965, 341.8683619253751], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -4.787, -4.987, -4.4575, -5.6712, -5.3964, -5.7229, -5.849, -5.1994, -5.0011, -6.0001, -6.0989, -5.8641, -6.3855, -5.7548, -5.0696, -6.4517, -5.2495, -5.4876, -6.3655, -6.5435, -6.5701, -6.5048, -6.2355, -6.5767, -6.283, -6.5312, -6.6798, -6.7638, -6.1374, -6.7606, -3.5188, -5.1052, -5.8881, -5.1297, -4.9567, -5.8766, -5.9839, -5.9615, -5.5613, -5.8465, -6.086, -5.1429, -5.4567, -5.7568, -5.6077, -5.3694, -5.5593, -5.4687, -4.6602, -5.3727, -5.6221, -5.5663, -5.2921, -5.1668, -4.985, -5.4914, -5.4269, -5.3193, -5.1237, -5.4759, -5.4127, -5.4513, -5.5323, -5.0854, -5.1515, -4.8589, -5.3927, -5.3824, -3.9909, -5.1432, -4.457, -5.9313, -5.9366, -5.5875, -5.6504, -6.1034, -5.5795, -5.8389, -5.8176, -6.2455, -6.2547, -6.2602, -6.021, -6.4416, -6.3169, -5.6677, -6.4349, -6.4796, -6.5813, -5.1885, -6.3429, -6.5168, -6.4128, -5.8922, -5.5519, -5.9031, -6.2657, -5.8234, -5.6249, -5.9725, -5.4989, -6.0189, -5.3846, -4.1732, -5.709, -5.134, -5.0353, -4.6398, -5.2212, -4.7519, -5.0242, -5.0394, -4.609, -4.9073, -5.1584, -4.6067, -4.7243, -5.0166, -5.296, -4.9641, -5.2307, -4.9974, -5.4189, -5.3286, -5.3135, -5.3229, -5.2752, -5.0903, -5.3818, -5.5318, -5.9501, -5.0381, -5.9869, -5.7009, -6.0084, -5.8377, -5.8799, -6.0864, -6.1267, -6.1351, -5.1944, -5.9068, -5.9582, -6.3383, -6.0749, -6.1282, -5.8778, -6.4258, -5.7104, -5.1358, -4.5653, -6.4879, -4.5176, -6.4544, -5.6989, -4.0619, -4.9266, -4.9565, -4.7888, -5.541, -5.5799, -5.4719, -4.8333, -5.6936, -4.6557, -5.1369, -4.3678, -4.7173, -5.6162, -3.9458, -4.9877, -5.5193, -5.1753, -5.3255, -5.1925, -5.2977, -4.7303, -4.4759, -4.482, -4.7258, -4.7758, -5.3508, -5.0926, -5.1665, -5.2187, -5.3013]}};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el351444524533281952135416\", ldavis_el351444524533281952135416_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el351444524533281952135416\", ldavis_el351444524533281952135416_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.5/d3.min.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.rawgit.com/bmabey/pyLDAvis/files/ldavis.v1.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el351444524533281952135416\", ldavis_el351444524533281952135416_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "PreparedData(topic_coordinates=            Freq  cluster  topics         x         y\n",
       "topic                                                \n",
       "2      39.169691        1       1 -0.119541  0.116747\n",
       "1      34.974303        1       2 -0.068205 -0.140163\n",
       "0      25.856006        1       3  0.187747  0.023416, topic_info=     Category         Freq        Term        Total  loglift  logprob\n",
       "term                                                                 \n",
       "825   Default  1213.000000       space  1213.000000  30.0000  30.0000\n",
       "344   Default   667.000000         god   667.000000  29.0000  29.0000\n",
       "327   Default   467.000000        game   467.000000  28.0000  28.0000\n",
       "994   Default   695.000000        year   695.000000  27.0000  27.0000\n",
       "433   Default   419.000000       jesus   419.000000  26.0000  26.0000\n",
       "567   Default   468.000000        nasa   468.000000  25.0000  25.0000\n",
       "882   Default   395.000000        team   395.000000  24.0000  24.0000\n",
       "328   Default   295.000000       games   295.000000  23.0000  23.0000\n",
       "79    Default   281.000000    baseball   281.000000  22.0000  22.0000\n",
       "469   Default   336.000000      launch   336.000000  21.0000  21.0000\n",
       "383   Default   280.000000         hit   280.000000  20.0000  20.0000\n",
       "94    Default   279.000000       bible   279.000000  19.0000  19.0000\n",
       "650   Default   229.000000     players   229.000000  18.0000  18.0000\n",
       "756   Default   222.000000        runs   222.000000  17.0000  17.0000\n",
       "979   Default   270.000000         won   270.000000  16.0000  16.0000\n",
       "797   Default   275.000000     shuttle   275.000000  15.0000  15.0000\n",
       "475   Default   198.000000      league   198.000000  14.0000  14.0000\n",
       "976   Default   192.000000         win   192.000000  13.0000  13.0000\n",
       "601   Default   273.000000       orbit   273.000000  12.0000  12.0000\n",
       "620   Default   745.000000      people   745.000000  11.0000  11.0000\n",
       "432   Default   223.000000     jehovah   223.000000  10.0000  10.0000\n",
       "780   Default   174.000000      season   174.000000   9.0000   9.0000\n",
       "208   Default   290.000000        data   290.000000   8.0000   8.0000\n",
       "561   Default   255.000000        moon   255.000000   7.0000   7.0000\n",
       "639   Default   164.000000    pitching   164.000000   6.0000   6.0000\n",
       "148   Default   208.000000      christ   208.000000   5.0000   5.0000\n",
       "149   Default   211.000000   christian   211.000000   4.0000   4.0000\n",
       "649   Default   159.000000      player   159.000000   3.0000   3.0000\n",
       "826   Default   249.000000  spacecraft   249.000000   2.0000   2.0000\n",
       "250   Default   484.000000       earth   484.000000   1.0000   1.0000\n",
       "...       ...          ...         ...          ...      ...      ...\n",
       "475    Topic3   192.583562      league   198.409803   1.3228  -4.9266\n",
       "976    Topic3   186.913725         win   192.671782   1.3223  -4.9565\n",
       "650    Topic3   221.046215     players   229.155917   1.3166  -4.7888\n",
       "852    Topic3   104.184888       stats   107.006471   1.3259  -5.5410\n",
       "385    Topic3   100.211355      hitter   102.871753   1.3264  -5.5799\n",
       "565    Topic3   111.634693      morris   115.064225   1.3224  -5.4719\n",
       "756    Topic3   211.428073        runs   222.788857   1.3003  -4.8333\n",
       "267    Topic3    89.439371         era    92.189955   1.3223  -5.6936\n",
       "383    Topic3   252.507307         hit   280.890304   1.2461  -4.6557\n",
       "647    Topic3   156.067761        play   169.845633   1.2680  -5.1369\n",
       "882    Topic3   336.756572        team   395.445424   1.1920  -4.3678\n",
       "979    Topic3   237.436263         won   270.360171   1.2228  -4.7173\n",
       "387    Topic3    96.642678     hitting   101.132472   1.3072  -5.6162\n",
       "994    Topic3   513.569440        year   695.776562   1.0490  -3.9458\n",
       "753    Topic3   181.169045         run   211.858380   1.1961  -4.9877\n",
       "883    Topic3   106.475727       teams   114.600317   1.2791  -5.5193\n",
       "71     Topic3   150.184993     average   173.785695   1.2067  -5.1753\n",
       "74     Topic3   129.244190        ball   145.825119   1.2319  -5.3255\n",
       "501    Topic3   147.620580        lost   172.915936   1.1945  -5.1925\n",
       "390    Topic3   132.889253        home   152.520475   1.2148  -5.2977\n",
       "93     Topic3   234.369335      better   413.046452   0.7860  -4.7303\n",
       "349    Topic3   302.253948        good   673.966222   0.5507  -4.4759\n",
       "899    Topic3   300.427711       think   798.800805   0.3747  -4.4820\n",
       "447    Topic3   235.430893        just   833.728589   0.0881  -4.7258\n",
       "908    Topic3   223.929690        time   741.828768   0.1548  -4.7758\n",
       "92     Topic3   126.009049        best   198.187550   0.8998  -5.3508\n",
       "456    Topic3   163.134479        know   680.253200  -0.0753  -5.0926\n",
       "229    Topic3   151.520623         did   484.184255   0.1909  -5.1665\n",
       "995    Topic3   143.810877       years   430.461561   0.2563  -5.2187\n",
       "710    Topic3   132.400824      really   341.868362   0.4040  -5.3013\n",
       "\n",
       "[216 rows x 6 columns], token_table=      Topic      Freq        Term\n",
       "term                             \n",
       "29        1  0.993956    altitude\n",
       "32        1  0.983887        ames\n",
       "32        2  0.014259        ames\n",
       "66        1  0.964663  atmosphere\n",
       "66        2  0.030383  atmosphere\n",
       "70        1  0.928604   available\n",
       "70        2  0.064555   available\n",
       "70        3  0.004966   available\n",
       "71        1  0.115084     average\n",
       "71        2  0.017263     average\n",
       "71        3  0.863132     average\n",
       "74        1  0.109720        ball\n",
       "74        2  0.006858        ball\n",
       "74        3  0.884621        ball\n",
       "79        1  0.014221    baseball\n",
       "79        2  0.003555    baseball\n",
       "79        3  0.981223    baseball\n",
       "87        1  0.011023     batting\n",
       "87        3  0.981056     batting\n",
       "89        2  0.992795     beliefs\n",
       "90        1  0.139915     believe\n",
       "90        2  0.652004     believe\n",
       "90        3  0.209873     believe\n",
       "92        1  0.211920        best\n",
       "92        2  0.151372        best\n",
       "92        3  0.635761        best\n",
       "93        1  0.193683      better\n",
       "93        2  0.239682      better\n",
       "93        3  0.566522      better\n",
       "94        1  0.003572       bible\n",
       "...     ...       ...         ...\n",
       "937       1  0.627439         use\n",
       "937       2  0.329250         use\n",
       "937       3  0.043486         use\n",
       "947       1  0.985085     vehicle\n",
       "947       2  0.011323     vehicle\n",
       "949       1  0.957973       venus\n",
       "949       2  0.038987       venus\n",
       "958       1  0.298647        want\n",
       "958       2  0.514500        want\n",
       "958       3  0.186285        want\n",
       "966       1  0.256516         way\n",
       "966       2  0.504553         way\n",
       "966       3  0.239557         way\n",
       "976       1  0.025951         win\n",
       "976       2  0.005190         win\n",
       "976       3  0.970562         win\n",
       "979       1  0.051783         won\n",
       "979       2  0.070277         won\n",
       "979       3  0.876608         won\n",
       "982       1  0.107199        word\n",
       "982       2  0.884391        word\n",
       "982       3  0.005360        word\n",
       "993       1  0.016128     yankees\n",
       "993       3  0.983837     yankees\n",
       "994       1  0.248643        year\n",
       "994       2  0.014372        year\n",
       "994       3  0.738743        year\n",
       "995       1  0.457648       years\n",
       "995       2  0.209078       years\n",
       "995       3  0.334525       years\n",
       "\n",
       "[377 rows x 3 columns], R=30, lambda_step=0.01, plot_opts={'xlab': 'PC1', 'ylab': 'PC2'}, topic_order=[3, 2, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "g_vectorizer = CountVectorizer(max_df=0.95, min_df=2, analyzer='word',\n",
    "    max_features=1000, stop_words=stop_words, token_pattern = r'\\b[a-zA-Z]{3,}\\b')\n",
    "\n",
    "fs = g_vectorizer.fit_transform(ds1.data)\n",
    "lda = LDA.fit(fs)\n",
    "    \n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.sklearn.prepare(lda, fs, g_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
